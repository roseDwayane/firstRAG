{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ee0f13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 150 entries\n"
     ]
    }
   ],
   "source": [
    "with open('cat-facts.txt', 'r', encoding='utf-8-sig') as file:\n",
    "    dataset = file.readlines()\n",
    "print(f'Loaded {len(dataset)} entries')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9095463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "EMBEDDING_MODEL = 'hf.co/CompendiumLabs/bge-base-en-v1.5-gguf'\n",
    "LANGUAGE_MODEL = 'hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF'\n",
    "\n",
    "# Each element in the VECTOR_DB will be a tuple (chunk, embedding)\n",
    "# The embedding is a list of floats, for example: [0.1, 0.04, -0.34, 0.21, ...]\n",
    "VECTOR_DB = []\n",
    "\n",
    "def add_chunk_to_database(chunk):\n",
    "  embedding = ollama.embed(model=EMBEDDING_MODEL, input=chunk)['embeddings'][0]\n",
    "  VECTOR_DB.append((chunk, embedding))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5ecfbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk 1/150 to the database\n",
      "Added chunk 2/150 to the database\n",
      "Added chunk 3/150 to the database\n",
      "Added chunk 4/150 to the database\n",
      "Added chunk 5/150 to the database\n",
      "Added chunk 6/150 to the database\n",
      "Added chunk 7/150 to the database\n",
      "Added chunk 8/150 to the database\n",
      "Added chunk 9/150 to the database\n",
      "Added chunk 10/150 to the database\n",
      "Added chunk 11/150 to the database\n",
      "Added chunk 12/150 to the database\n",
      "Added chunk 13/150 to the database\n",
      "Added chunk 14/150 to the database\n",
      "Added chunk 15/150 to the database\n",
      "Added chunk 16/150 to the database\n",
      "Added chunk 17/150 to the database\n",
      "Added chunk 18/150 to the database\n",
      "Added chunk 19/150 to the database\n",
      "Added chunk 20/150 to the database\n",
      "Added chunk 21/150 to the database\n",
      "Added chunk 22/150 to the database\n",
      "Added chunk 23/150 to the database\n",
      "Added chunk 24/150 to the database\n",
      "Added chunk 25/150 to the database\n",
      "Added chunk 26/150 to the database\n",
      "Added chunk 27/150 to the database\n",
      "Added chunk 28/150 to the database\n",
      "Added chunk 29/150 to the database\n",
      "Added chunk 30/150 to the database\n",
      "Added chunk 31/150 to the database\n",
      "Added chunk 32/150 to the database\n",
      "Added chunk 33/150 to the database\n",
      "Added chunk 34/150 to the database\n",
      "Added chunk 35/150 to the database\n",
      "Added chunk 36/150 to the database\n",
      "Added chunk 37/150 to the database\n",
      "Added chunk 38/150 to the database\n",
      "Added chunk 39/150 to the database\n",
      "Added chunk 40/150 to the database\n",
      "Added chunk 41/150 to the database\n",
      "Added chunk 42/150 to the database\n",
      "Added chunk 43/150 to the database\n",
      "Added chunk 44/150 to the database\n",
      "Added chunk 45/150 to the database\n",
      "Added chunk 46/150 to the database\n",
      "Added chunk 47/150 to the database\n",
      "Added chunk 48/150 to the database\n",
      "Added chunk 49/150 to the database\n",
      "Added chunk 50/150 to the database\n",
      "Added chunk 51/150 to the database\n",
      "Added chunk 52/150 to the database\n",
      "Added chunk 53/150 to the database\n",
      "Added chunk 54/150 to the database\n",
      "Added chunk 55/150 to the database\n",
      "Added chunk 56/150 to the database\n",
      "Added chunk 57/150 to the database\n",
      "Added chunk 58/150 to the database\n",
      "Added chunk 59/150 to the database\n",
      "Added chunk 60/150 to the database\n",
      "Added chunk 61/150 to the database\n",
      "Added chunk 62/150 to the database\n",
      "Added chunk 63/150 to the database\n",
      "Added chunk 64/150 to the database\n",
      "Added chunk 65/150 to the database\n",
      "Added chunk 66/150 to the database\n",
      "Added chunk 67/150 to the database\n",
      "Added chunk 68/150 to the database\n",
      "Added chunk 69/150 to the database\n",
      "Added chunk 70/150 to the database\n",
      "Added chunk 71/150 to the database\n",
      "Added chunk 72/150 to the database\n",
      "Added chunk 73/150 to the database\n",
      "Added chunk 74/150 to the database\n",
      "Added chunk 75/150 to the database\n",
      "Added chunk 76/150 to the database\n",
      "Added chunk 77/150 to the database\n",
      "Added chunk 78/150 to the database\n",
      "Added chunk 79/150 to the database\n",
      "Added chunk 80/150 to the database\n",
      "Added chunk 81/150 to the database\n",
      "Added chunk 82/150 to the database\n",
      "Added chunk 83/150 to the database\n",
      "Added chunk 84/150 to the database\n",
      "Added chunk 85/150 to the database\n",
      "Added chunk 86/150 to the database\n",
      "Added chunk 87/150 to the database\n",
      "Added chunk 88/150 to the database\n",
      "Added chunk 89/150 to the database\n",
      "Added chunk 90/150 to the database\n",
      "Added chunk 91/150 to the database\n",
      "Added chunk 92/150 to the database\n",
      "Added chunk 93/150 to the database\n",
      "Added chunk 94/150 to the database\n",
      "Added chunk 95/150 to the database\n",
      "Added chunk 96/150 to the database\n",
      "Added chunk 97/150 to the database\n",
      "Added chunk 98/150 to the database\n",
      "Added chunk 99/150 to the database\n",
      "Added chunk 100/150 to the database\n",
      "Added chunk 101/150 to the database\n",
      "Added chunk 102/150 to the database\n",
      "Added chunk 103/150 to the database\n",
      "Added chunk 104/150 to the database\n",
      "Added chunk 105/150 to the database\n",
      "Added chunk 106/150 to the database\n",
      "Added chunk 107/150 to the database\n",
      "Added chunk 108/150 to the database\n",
      "Added chunk 109/150 to the database\n",
      "Added chunk 110/150 to the database\n",
      "Added chunk 111/150 to the database\n",
      "Added chunk 112/150 to the database\n",
      "Added chunk 113/150 to the database\n",
      "Added chunk 114/150 to the database\n",
      "Added chunk 115/150 to the database\n",
      "Added chunk 116/150 to the database\n",
      "Added chunk 117/150 to the database\n",
      "Added chunk 118/150 to the database\n",
      "Added chunk 119/150 to the database\n",
      "Added chunk 120/150 to the database\n",
      "Added chunk 121/150 to the database\n",
      "Added chunk 122/150 to the database\n",
      "Added chunk 123/150 to the database\n",
      "Added chunk 124/150 to the database\n",
      "Added chunk 125/150 to the database\n",
      "Added chunk 126/150 to the database\n",
      "Added chunk 127/150 to the database\n",
      "Added chunk 128/150 to the database\n",
      "Added chunk 129/150 to the database\n",
      "Added chunk 130/150 to the database\n",
      "Added chunk 131/150 to the database\n",
      "Added chunk 132/150 to the database\n",
      "Added chunk 133/150 to the database\n",
      "Added chunk 134/150 to the database\n",
      "Added chunk 135/150 to the database\n",
      "Added chunk 136/150 to the database\n",
      "Added chunk 137/150 to the database\n",
      "Added chunk 138/150 to the database\n",
      "Added chunk 139/150 to the database\n",
      "Added chunk 140/150 to the database\n",
      "Added chunk 141/150 to the database\n",
      "Added chunk 142/150 to the database\n",
      "Added chunk 143/150 to the database\n",
      "Added chunk 144/150 to the database\n",
      "Added chunk 145/150 to the database\n",
      "Added chunk 146/150 to the database\n",
      "Added chunk 147/150 to the database\n",
      "Added chunk 148/150 to the database\n",
      "Added chunk 149/150 to the database\n",
      "Added chunk 150/150 to the database\n"
     ]
    }
   ],
   "source": [
    "for i, chunk in enumerate(dataset):\n",
    "  add_chunk_to_database(chunk)\n",
    "  print(f'Added chunk {i+1}/{len(dataset)} to the database')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daf9cca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "  dot_product = sum([x * y for x, y in zip(a, b)])\n",
    "  norm_a = sum([x ** 2 for x in a]) ** 0.5\n",
    "  norm_b = sum([x ** 2 for x in b]) ** 0.5\n",
    "  return dot_product / (norm_a * norm_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a39e4bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, top_n=3):\n",
    "  query_embedding = ollama.embed(model=EMBEDDING_MODEL, input=query)['embeddings'][0]\n",
    "  # temporary list to store (chunk, similarity) pairs\n",
    "  similarities = []\n",
    "  for chunk, embedding in VECTOR_DB:\n",
    "    similarity = cosine_similarity(query_embedding, embedding)\n",
    "    similarities.append((chunk, similarity))\n",
    "  # sort by similarity in descending order, because higher similarity means more relevant chunks\n",
    "  similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "  # finally, return the top N most relevant chunks\n",
    "  return similarities[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dfdb4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved knowledge:\n",
      " - (similarity: 0.76) In ancient Egypt, mummies were made of cats, and embalmed mice were placed with them in their tombs. In one ancient city, over 300,000 cat mummies were found.\n",
      "\n",
      " - (similarity: 0.72) In 1888, more than 300,000 mummified cats were found an Egyptian cemetery. They were stripped of their wrappings and carted off to be used by farmers in England and the U.S. for fertilizer.\n",
      "\n",
      " - (similarity: 0.65) When a family cat died in ancient Egypt, family members would mourn by shaving off their eyebrows. They also held elaborate funerals during which they drank wine and beat their breasts. The cat was embalmed with a sculpted wooden mask and the tiny mummy was placed in the family tomb or in a pet cemetery with tiny mummies of mice.\n",
      "\n",
      "You are a helpful chatbot.\n",
      "Use only the following pieces of context to answer the question. Don't make up any new information:\n",
      " - In ancient Egypt, mummies were made of cats, and embalmed mice were placed with them in their tombs. In one ancient city, over 300,000 cat mummies were found.\n",
      "\n",
      " - In 1888, more than 300,000 mummified cats were found an Egyptian cemetery. They were stripped of their wrappings and carted off to be used by farmers in England and the U.S. for fertilizer.\n",
      "\n",
      " - When a family cat died in ancient Egypt, family members would mourn by shaving off their eyebrows. They also held elaborate funerals during which they drank wine and beat their breasts. The cat was embalmed with a sculpted wooden mask and the tiny mummy was placed in the family tomb or in a pet cemetery with tiny mummies of mice.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_query = input('Ask me a question: ')\n",
    "retrieved_knowledge = retrieve(input_query)\n",
    "\n",
    "print('Retrieved knowledge:')\n",
    "for chunk, similarity in retrieved_knowledge:\n",
    "    print(f' - (similarity: {similarity:.2f}) {chunk}')\n",
    "\n",
    "# 先組好上下文字串（注意這裡沒有 f-string）\n",
    "context = \"\\n\".join([f\" - {chunk}\" for chunk, _ in retrieved_knowledge])\n",
    "\n",
    "# 再把變數插入到多行字串\n",
    "instruction_prompt = (\n",
    "    \"You are a helpful chatbot.\\n\"\n",
    "    \"Use only the following pieces of context to answer the question. \"\n",
    "    \"Don't make up any new information:\\n\"\n",
    "    f\"{context}\"\n",
    ")\n",
    "\n",
    "print(instruction_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63d0d225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot response:\n",
      "The ancient Egyptians believed that cats were sacred animals, and they often mummified them as part of their funerary rites. The mummified cats were placed in tombs or cemeteries with other family members who had died, and were sometimes even used for fertilizer.\n",
      "\n",
      "One notable example is the discovery of over 300,000 cat mummies in a single Egyptian cemetery, which was found in one ancient city. The mummies were typically buried with small tokens, such as tiny figurines or trinkets, that represented the deceased cat's status within their family and community.\n",
      "\n",
      "In addition to being used for funerary purposes, the cat mummified animals could also be used by farmers in ancient Egypt who needed a source of fertilizer. This practice was known as \"dung farming,\" where animal manure was collected from farms and used as a natural fertilizer."
     ]
    }
   ],
   "source": [
    "stream = ollama.chat(\n",
    "  model=LANGUAGE_MODEL,\n",
    "  messages=[\n",
    "    {'role': 'system', 'content': instruction_prompt},\n",
    "    {'role': 'user', 'content': input_query},\n",
    "  ],\n",
    "  stream=True,\n",
    ")\n",
    "\n",
    "# print the response from the chatbot in real-time\n",
    "print('Chatbot response:')\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3fe148a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chang\\anaconda3\\envs\\rag\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Chang\\anaconda3\\envs\\rag\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Chang\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['段落3...', '段落1...', '段落2...']\n"
     ]
    }
   ],
   "source": [
    "# pip install faiss-cpu sentence-transformers\n",
    "import numpy as np, faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "docs = [\"段落1...\", \"段落2...\", \"段落3...\"]\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "emb = model.encode(docs, normalize_embeddings=True).astype(\"float32\")  # cosine→先正規化\n",
    "d = emb.shape[1]\n",
    "\n",
    "index = faiss.IndexIDMap(faiss.IndexFlatIP(d))  # 用內積=cosine\n",
    "ids = np.arange(len(docs)).astype(\"int64\")\n",
    "index.add_with_ids(emb, ids)\n",
    "\n",
    "q = model.encode([\"我的問題是...\"], normalize_embeddings=True).astype(\"float32\")\n",
    "D, I = index.search(q, k=3)  # 取 Top-3\n",
    "print([docs[i] for i in I[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42e9829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 265 rows\n",
      "index.ntotal = 265\n",
      "0.6713 | InvoicePayment: received. AccountingCategory: 資本. Summary: 張功爾 | {'InvoicePayment': 'received', 'AccountingCategory': '資本', 'Summary': '張功爾'}\n",
      "0.6537 | InvoicePayment: paid. AccountingCategory: 薪資支出. Summary: 12月薪資張功爾 | {'InvoicePayment': 'paid', 'AccountingCategory': '薪資支出', 'Summary': '12月薪資張功爾'}\n",
      "0.6531 | InvoicePayment: paid. AccountingCategory: 銀行存款. Summary: 張功爾 | {'InvoicePayment': 'paid', 'AccountingCategory': '銀行存款', 'Summary': '張功爾'}\n"
     ]
    }
   ],
   "source": [
    "# pip install faiss-cpu sentence-transformers  # 先裝好（即便用 Ollama，也建議裝供日後切換）\n",
    "\n",
    "import csv, numpy as np, faiss, os\n",
    "from textwrap import dedent\n",
    "\n",
    "# ==== 0) 開關：要用哪個向量模型來「生 embedding」？ ====\n",
    "USE_OLLAMA = False   # True=沿用你的 Ollama bge-base-zh；False=改用 sentence-transformers\n",
    "\n",
    "if USE_OLLAMA:\n",
    "    import ollama\n",
    "    EMBEDDING_MODEL = 'hf.co/CompendiumLabs/bge-base-zh-v1.5-gguf'  # 你原本的\n",
    "else:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    # 建議中文/多語模型（all-MiniLM-L6-v2 偏英文）\n",
    "    EMBEDDING_MODEL = 'BAAI/bge-m3'  # 或 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "    st_model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "\n",
    "# ==== 1) 讀取表格 ====\n",
    "ROWS = []\n",
    "with open('records.csv', newline='', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for r in reader:\n",
    "        pay  = (r.get('InvoicePayment', '') or '').strip()\n",
    "        cat  = (r.get('AccountingCategory', '') or '').strip()\n",
    "        summ = (r.get('Summary', '') or '').strip()\n",
    "        if not pay and not cat and not summ:\n",
    "            continue\n",
    "        ROWS.append({'InvoicePayment': pay, 'AccountingCategory': cat, 'Summary': summ})\n",
    "\n",
    "print(f'Loaded {len(ROWS)} rows')\n",
    "\n",
    "# ==== 2) 準備要嵌入的文字（把多欄位組成一段） ====\n",
    "DOCS = []\n",
    "for row in ROWS:\n",
    "    text_for_embedding = (\n",
    "        f\"InvoicePayment: {row['InvoicePayment']}. \"\n",
    "        f\"AccountingCategory: {row['AccountingCategory']}. \"\n",
    "        f\"Summary: {row['Summary']}\"\n",
    "    )\n",
    "    DOCS.append(text_for_embedding)\n",
    "\n",
    "# ==== 3) 產生向量 ====\n",
    "def embed_texts(texts):\n",
    "    if USE_OLLAMA:\n",
    "        # Ollama 一次一段；也可自行批次呼叫以降低 overhead\n",
    "        vecs = []\n",
    "        for t in texts:\n",
    "            emb = ollama.embed(model=EMBEDDING_MODEL, input=t)['embeddings'][0]\n",
    "            vecs.append(emb)\n",
    "        arr = np.array(vecs, dtype='float32')\n",
    "    else:\n",
    "        # sentence-transformers 可一次批量編碼，normalize_embeddings=True → 直接用 cosine\n",
    "        arr = st_model.encode(texts, normalize_embeddings=True).astype('float32')\n",
    "    # 若用 Ollama，建議自行做 L2 normalize 以便用 IP 當 cosine\n",
    "    #（用 sbert 已經正規化過則不會改變值）\n",
    "    norms = np.linalg.norm(arr, axis=1, keepdims=True) + 1e-12\n",
    "    arr = arr / norms\n",
    "    return arr\n",
    "\n",
    "EMB = embed_texts(DOCS).astype('float32')\n",
    "d = EMB.shape[1]\n",
    "\n",
    "# ==== 4) 建 FAISS 索引（用內積，等價 cosine；並保留 id 對應） ====\n",
    "index = faiss.IndexIDMap(faiss.IndexFlatIP(d))\n",
    "ids = np.arange(len(DOCS)).astype('int64')\n",
    "index.add_with_ids(EMB, ids)\n",
    "print(\"index.ntotal =\", index.ntotal)\n",
    "\n",
    "# 為了能回傳原欄位，做一個 id → metadata 的對照表\n",
    "ID2META = {i: ROWS[i] for i in range(len(ROWS))}\n",
    "ID2TEXT = {i: DOCS[i] for i in range(len(DOCS))}\n",
    "\n",
    "# ==== 5) 查詢函式（輸入問題字串，回傳 Top-k + 分數 + 原欄位） ====\n",
    "def search(query_text, k=5):\n",
    "    q = embed_texts([query_text]).astype('float32')\n",
    "    D, I = index.search(q, k)\n",
    "    results = []\n",
    "    for score, idx in zip(D[0], I[0]):\n",
    "        if idx == -1:  # 當 k 超過資料量時，FAISS 可能回 -1\n",
    "            continue\n",
    "        results.append({\n",
    "            'score': float(score),            # cosine 相似度\n",
    "            'text': ID2TEXT[int(idx)],\n",
    "            'meta': ID2META[int(idx)]\n",
    "        })\n",
    "    return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7135fe37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6713 | InvoicePayment: received. AccountingCategory: 資本. Summary: 張功爾 | {'InvoicePayment': 'received', 'AccountingCategory': '資本', 'Summary': '張功爾'}\n",
      "0.6537 | InvoicePayment: paid. AccountingCategory: 薪資支出. Summary: 12月薪資張功爾 | {'InvoicePayment': 'paid', 'AccountingCategory': '薪資支出', 'Summary': '12月薪資張功爾'}\n",
      "0.6531 | InvoicePayment: paid. AccountingCategory: 銀行存款. Summary: 張功爾 | {'InvoicePayment': 'paid', 'AccountingCategory': '銀行存款', 'Summary': '張功爾'}\n"
     ]
    }
   ],
   "source": [
    "# ==== 6) 小測試 ====\n",
    "if __name__ == \"__main__\":\n",
    "    demo_q = \"張功爾 paid\"\n",
    "    rst = search(demo_q, k=3)\n",
    "    for r in rst:\n",
    "        print(f\"{r['score']:.4f} | {r['text']} | {r['meta']}\")\n",
    "\n",
    "    #（可選）持久化索引\n",
    "    faiss.write_index(index, \"records.faiss\")\n",
    "    # 讀回時： index = faiss.read_index(\"records.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedd23c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
