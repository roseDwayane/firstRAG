{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ee0f13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 150 entries\n"
     ]
    }
   ],
   "source": [
    "with open('cat-facts.txt', 'r', encoding='utf-8-sig') as file:\n",
    "    dataset = file.readlines()\n",
    "print(f'Loaded {len(dataset)} entries')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9095463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "EMBEDDING_MODEL = 'hf.co/CompendiumLabs/bge-base-en-v1.5-gguf'\n",
    "LANGUAGE_MODEL = 'hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF'\n",
    "\n",
    "# Each element in the VECTOR_DB will be a tuple (chunk, embedding)\n",
    "# The embedding is a list of floats, for example: [0.1, 0.04, -0.34, 0.21, ...]\n",
    "VECTOR_DB = []\n",
    "\n",
    "def add_chunk_to_database(chunk):\n",
    "  embedding = ollama.embed(model=EMBEDDING_MODEL, input=chunk)['embeddings'][0]\n",
    "  VECTOR_DB.append((chunk, embedding))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5ecfbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added chunk 1/150 to the database\n",
      "Added chunk 2/150 to the database\n",
      "Added chunk 3/150 to the database\n",
      "Added chunk 4/150 to the database\n",
      "Added chunk 5/150 to the database\n",
      "Added chunk 6/150 to the database\n",
      "Added chunk 7/150 to the database\n",
      "Added chunk 8/150 to the database\n",
      "Added chunk 9/150 to the database\n",
      "Added chunk 10/150 to the database\n",
      "Added chunk 11/150 to the database\n",
      "Added chunk 12/150 to the database\n",
      "Added chunk 13/150 to the database\n",
      "Added chunk 14/150 to the database\n",
      "Added chunk 15/150 to the database\n",
      "Added chunk 16/150 to the database\n",
      "Added chunk 17/150 to the database\n",
      "Added chunk 18/150 to the database\n",
      "Added chunk 19/150 to the database\n",
      "Added chunk 20/150 to the database\n",
      "Added chunk 21/150 to the database\n",
      "Added chunk 22/150 to the database\n",
      "Added chunk 23/150 to the database\n",
      "Added chunk 24/150 to the database\n",
      "Added chunk 25/150 to the database\n",
      "Added chunk 26/150 to the database\n",
      "Added chunk 27/150 to the database\n",
      "Added chunk 28/150 to the database\n",
      "Added chunk 29/150 to the database\n",
      "Added chunk 30/150 to the database\n",
      "Added chunk 31/150 to the database\n",
      "Added chunk 32/150 to the database\n",
      "Added chunk 33/150 to the database\n",
      "Added chunk 34/150 to the database\n",
      "Added chunk 35/150 to the database\n",
      "Added chunk 36/150 to the database\n",
      "Added chunk 37/150 to the database\n",
      "Added chunk 38/150 to the database\n",
      "Added chunk 39/150 to the database\n",
      "Added chunk 40/150 to the database\n",
      "Added chunk 41/150 to the database\n",
      "Added chunk 42/150 to the database\n",
      "Added chunk 43/150 to the database\n",
      "Added chunk 44/150 to the database\n",
      "Added chunk 45/150 to the database\n",
      "Added chunk 46/150 to the database\n",
      "Added chunk 47/150 to the database\n",
      "Added chunk 48/150 to the database\n",
      "Added chunk 49/150 to the database\n",
      "Added chunk 50/150 to the database\n",
      "Added chunk 51/150 to the database\n",
      "Added chunk 52/150 to the database\n",
      "Added chunk 53/150 to the database\n",
      "Added chunk 54/150 to the database\n",
      "Added chunk 55/150 to the database\n",
      "Added chunk 56/150 to the database\n",
      "Added chunk 57/150 to the database\n",
      "Added chunk 58/150 to the database\n",
      "Added chunk 59/150 to the database\n",
      "Added chunk 60/150 to the database\n",
      "Added chunk 61/150 to the database\n",
      "Added chunk 62/150 to the database\n",
      "Added chunk 63/150 to the database\n",
      "Added chunk 64/150 to the database\n",
      "Added chunk 65/150 to the database\n",
      "Added chunk 66/150 to the database\n",
      "Added chunk 67/150 to the database\n",
      "Added chunk 68/150 to the database\n",
      "Added chunk 69/150 to the database\n",
      "Added chunk 70/150 to the database\n",
      "Added chunk 71/150 to the database\n",
      "Added chunk 72/150 to the database\n",
      "Added chunk 73/150 to the database\n",
      "Added chunk 74/150 to the database\n",
      "Added chunk 75/150 to the database\n",
      "Added chunk 76/150 to the database\n",
      "Added chunk 77/150 to the database\n",
      "Added chunk 78/150 to the database\n",
      "Added chunk 79/150 to the database\n",
      "Added chunk 80/150 to the database\n",
      "Added chunk 81/150 to the database\n",
      "Added chunk 82/150 to the database\n",
      "Added chunk 83/150 to the database\n",
      "Added chunk 84/150 to the database\n",
      "Added chunk 85/150 to the database\n",
      "Added chunk 86/150 to the database\n",
      "Added chunk 87/150 to the database\n",
      "Added chunk 88/150 to the database\n",
      "Added chunk 89/150 to the database\n",
      "Added chunk 90/150 to the database\n",
      "Added chunk 91/150 to the database\n",
      "Added chunk 92/150 to the database\n",
      "Added chunk 93/150 to the database\n",
      "Added chunk 94/150 to the database\n",
      "Added chunk 95/150 to the database\n",
      "Added chunk 96/150 to the database\n",
      "Added chunk 97/150 to the database\n",
      "Added chunk 98/150 to the database\n",
      "Added chunk 99/150 to the database\n",
      "Added chunk 100/150 to the database\n",
      "Added chunk 101/150 to the database\n",
      "Added chunk 102/150 to the database\n",
      "Added chunk 103/150 to the database\n",
      "Added chunk 104/150 to the database\n",
      "Added chunk 105/150 to the database\n",
      "Added chunk 106/150 to the database\n",
      "Added chunk 107/150 to the database\n",
      "Added chunk 108/150 to the database\n",
      "Added chunk 109/150 to the database\n",
      "Added chunk 110/150 to the database\n",
      "Added chunk 111/150 to the database\n",
      "Added chunk 112/150 to the database\n",
      "Added chunk 113/150 to the database\n",
      "Added chunk 114/150 to the database\n",
      "Added chunk 115/150 to the database\n",
      "Added chunk 116/150 to the database\n",
      "Added chunk 117/150 to the database\n",
      "Added chunk 118/150 to the database\n",
      "Added chunk 119/150 to the database\n",
      "Added chunk 120/150 to the database\n",
      "Added chunk 121/150 to the database\n",
      "Added chunk 122/150 to the database\n",
      "Added chunk 123/150 to the database\n",
      "Added chunk 124/150 to the database\n",
      "Added chunk 125/150 to the database\n",
      "Added chunk 126/150 to the database\n",
      "Added chunk 127/150 to the database\n",
      "Added chunk 128/150 to the database\n",
      "Added chunk 129/150 to the database\n",
      "Added chunk 130/150 to the database\n",
      "Added chunk 131/150 to the database\n",
      "Added chunk 132/150 to the database\n",
      "Added chunk 133/150 to the database\n",
      "Added chunk 134/150 to the database\n",
      "Added chunk 135/150 to the database\n",
      "Added chunk 136/150 to the database\n",
      "Added chunk 137/150 to the database\n",
      "Added chunk 138/150 to the database\n",
      "Added chunk 139/150 to the database\n",
      "Added chunk 140/150 to the database\n",
      "Added chunk 141/150 to the database\n",
      "Added chunk 142/150 to the database\n",
      "Added chunk 143/150 to the database\n",
      "Added chunk 144/150 to the database\n",
      "Added chunk 145/150 to the database\n",
      "Added chunk 146/150 to the database\n",
      "Added chunk 147/150 to the database\n",
      "Added chunk 148/150 to the database\n",
      "Added chunk 149/150 to the database\n",
      "Added chunk 150/150 to the database\n"
     ]
    }
   ],
   "source": [
    "for i, chunk in enumerate(dataset):\n",
    "  add_chunk_to_database(chunk)\n",
    "  print(f'Added chunk {i+1}/{len(dataset)} to the database')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daf9cca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "  dot_product = sum([x * y for x, y in zip(a, b)])\n",
    "  norm_a = sum([x ** 2 for x in a]) ** 0.5\n",
    "  norm_b = sum([x ** 2 for x in b]) ** 0.5\n",
    "  return dot_product / (norm_a * norm_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a39e4bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, top_n=3):\n",
    "  query_embedding = ollama.embed(model=EMBEDDING_MODEL, input=query)['embeddings'][0]\n",
    "  # temporary list to store (chunk, similarity) pairs\n",
    "  similarities = []\n",
    "  for chunk, embedding in VECTOR_DB:\n",
    "    similarity = cosine_similarity(query_embedding, embedding)\n",
    "    similarities.append((chunk, similarity))\n",
    "  # sort by similarity in descending order, because higher similarity means more relevant chunks\n",
    "  similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "  # finally, return the top N most relevant chunks\n",
    "  return similarities[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dfdb4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved knowledge:\n",
      " - (similarity: 0.76) In ancient Egypt, mummies were made of cats, and embalmed mice were placed with them in their tombs. In one ancient city, over 300,000 cat mummies were found.\n",
      "\n",
      " - (similarity: 0.72) In 1888, more than 300,000 mummified cats were found an Egyptian cemetery. They were stripped of their wrappings and carted off to be used by farmers in England and the U.S. for fertilizer.\n",
      "\n",
      " - (similarity: 0.65) When a family cat died in ancient Egypt, family members would mourn by shaving off their eyebrows. They also held elaborate funerals during which they drank wine and beat their breasts. The cat was embalmed with a sculpted wooden mask and the tiny mummy was placed in the family tomb or in a pet cemetery with tiny mummies of mice.\n",
      "\n",
      "You are a helpful chatbot.\n",
      "Use only the following pieces of context to answer the question. Don't make up any new information:\n",
      " - In ancient Egypt, mummies were made of cats, and embalmed mice were placed with them in their tombs. In one ancient city, over 300,000 cat mummies were found.\n",
      "\n",
      " - In 1888, more than 300,000 mummified cats were found an Egyptian cemetery. They were stripped of their wrappings and carted off to be used by farmers in England and the U.S. for fertilizer.\n",
      "\n",
      " - When a family cat died in ancient Egypt, family members would mourn by shaving off their eyebrows. They also held elaborate funerals during which they drank wine and beat their breasts. The cat was embalmed with a sculpted wooden mask and the tiny mummy was placed in the family tomb or in a pet cemetery with tiny mummies of mice.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_query = input('Ask me a question: ')\n",
    "retrieved_knowledge = retrieve(input_query)\n",
    "\n",
    "print('Retrieved knowledge:')\n",
    "for chunk, similarity in retrieved_knowledge:\n",
    "    print(f' - (similarity: {similarity:.2f}) {chunk}')\n",
    "\n",
    "# 先組好上下文字串（注意這裡沒有 f-string）\n",
    "context = \"\\n\".join([f\" - {chunk}\" for chunk, _ in retrieved_knowledge])\n",
    "\n",
    "# 再把變數插入到多行字串\n",
    "instruction_prompt = (\n",
    "    \"You are a helpful chatbot.\\n\"\n",
    "    \"Use only the following pieces of context to answer the question. \"\n",
    "    \"Don't make up any new information:\\n\"\n",
    "    f\"{context}\"\n",
    ")\n",
    "\n",
    "print(instruction_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63d0d225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot response:\n",
      "The ancient Egyptians believed that cats were sacred animals, and they often mummified them as part of their funerary rites. The mummified cats were placed in tombs or cemeteries with other family members who had died, and were sometimes even used for fertilizer.\n",
      "\n",
      "One notable example is the discovery of over 300,000 cat mummies in a single Egyptian cemetery, which was found in one ancient city. The mummies were typically buried with small tokens, such as tiny figurines or trinkets, that represented the deceased cat's status within their family and community.\n",
      "\n",
      "In addition to being used for funerary purposes, the cat mummified animals could also be used by farmers in ancient Egypt who needed a source of fertilizer. This practice was known as \"dung farming,\" where animal manure was collected from farms and used as a natural fertilizer."
     ]
    }
   ],
   "source": [
    "stream = ollama.chat(\n",
    "  model=LANGUAGE_MODEL,\n",
    "  messages=[\n",
    "    {'role': 'system', 'content': instruction_prompt},\n",
    "    {'role': 'user', 'content': input_query},\n",
    "  ],\n",
    "  stream=True,\n",
    ")\n",
    "\n",
    "# print the response from the chatbot in real-time\n",
    "print('Chatbot response:')\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
